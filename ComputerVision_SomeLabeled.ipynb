{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ComputerVision_SomeLabeled.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kw25IdxMhWBR"
      },
      "source": [
        "**Run experiments on CIFAR and SVHN using RandAugment, the Wide-ResNet-28-2 model, the loss from [UDA](https://arxiv.org/pdf/1904.12848.pdf), and treating only a portion of the CIFAR and SVHN images as labeled**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PCMYd7lWazUR"
      },
      "source": [
        "import torch\r\n",
        "import torch.nn as nn\r\n",
        "from torchvision import datasets\r\n",
        "from torchvision import transforms\r\n",
        "import numpy as np\r\n",
        "import torch.nn.functional as F\r\n",
        "import torch.optim as optim\r\n",
        "import random\r\n",
        "import math\r\n",
        "from PIL import Image\r\n",
        "import PIL.ImageOps\r\n",
        "import PIL.ImageEnhance\r\n",
        "import PIL.ImageFilter"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i0Jc8EBAuXmy"
      },
      "source": [
        "**Original RandAugment**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t4q1IkOXRDIC"
      },
      "source": [
        "TRANSFORMATIONS = ['invert', 'cutout', 'sharpness', 'autocontrast', 'posterize', 'shearx', 'translatex', 'translatey',\r\n",
        "                   'sheary', 'rotate', 'equalize', 'contrast', 'color', 'solarize', 'brightness']"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qEs7c7KxXVoz"
      },
      "source": [
        "def transform_image(image, transform, magnitude):\r\n",
        "    \"\"\"Applies the transform to the given image with the given magnitude if the\r\n",
        "    transformation takes a magnitude.\r\n",
        "\r\n",
        "    Args:\r\n",
        "        image: The image to transform.\r\n",
        "        transform: The transformation to apply.\r\n",
        "        magnitude: The magnitude of the transformation, if applicable.\r\n",
        "    Returns:\r\n",
        "        The transformed image.\r\n",
        "    \"\"\"\r\n",
        "    if transform == 'invert':\r\n",
        "        return PIL.ImageOps.invert(image)\r\n",
        "    elif transform == 'cutout':\r\n",
        "        crop_box = (magnitude, magnitude, image.width - magnitude, image.height - magnitude)\r\n",
        "        return image.crop(box=crop_box).resize(image.size)\r\n",
        "    elif transform == 'sharpness':\r\n",
        "        enhancer = PIL.ImageEnhance.Sharpness(image)\r\n",
        "        return enhancer.enhance(magnitude)\r\n",
        "    elif transform == 'autocontrast':\r\n",
        "        return PIL.ImageOps.autocontrast(image, cutoff=magnitude)\r\n",
        "    elif transform == 'posterize':\r\n",
        "        return PIL.ImageOps.posterize(image, min(magnitude, 8))\r\n",
        "    elif transform == 'shearx':\r\n",
        "        transform_values = (1, magnitude, -1 * magnitude * image.width, 0, 1, 0)\r\n",
        "        return image.transform(image.size, Image.AFFINE, transform_values)\r\n",
        "    elif transform == 'translatex':\r\n",
        "        return image.transform(image.size, Image.AFFINE, (1, 0, magnitude, 0, 1, 0))\r\n",
        "    elif transform == 'translatey':\r\n",
        "        return image.transform(image.size, Image.AFFINE, (1, 0, 0, 0, 1, magnitude))\r\n",
        "    elif transform == 'sheary':\r\n",
        "        transform_values = (1, 0, 0, magnitude, 1, -1 * magnitude * image.height)\r\n",
        "        return image.transform(image.size, Image.AFFINE, transform_values)\r\n",
        "    elif transform == 'rotate':\r\n",
        "        return image.rotate(magnitude)\r\n",
        "    elif transform == 'equalize':\r\n",
        "        return PIL.ImageOps.equalize(image)\r\n",
        "    elif transform == 'contrast':\r\n",
        "        enhancer = PIL.ImageEnhance.Contrast(image)\r\n",
        "        return enhancer.enhance(magnitude)\r\n",
        "    elif transform == 'color':\r\n",
        "        enhancer = PIL.ImageEnhance.Color(image)\r\n",
        "        return enhancer.enhance(magnitude)\r\n",
        "    elif transform == 'solarize':\r\n",
        "        return PIL.ImageOps.solarize(image, magnitude)\r\n",
        "    else:\r\n",
        "        # brightness\r\n",
        "        enhancer = PIL.ImageEnhance.Brightness(image)\r\n",
        "        return enhancer.enhance(magnitude)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "90DAk-pZKLwi"
      },
      "source": [
        "def rand_augment(image, probability=0.5):\r\n",
        "    \"\"\"Applies RandAugment to the given image (a tensor) with the given\r\n",
        "    probability being the probability with which to apply the transformation.\r\n",
        "\r\n",
        "    Args:\r\n",
        "        image: The image to transform.\r\n",
        "        probability: The probability with which to transform the image.\r\n",
        "    Returns:\r\n",
        "        The transformed image.\r\n",
        "    \"\"\"\r\n",
        "    if np.random.binomial(1, 0.5) == 0:\r\n",
        "        return torch.clone(image)\r\n",
        "    cur_transform = random.choice(TRANSFORMATIONS)\r\n",
        "    magnitude = random.randint(1, 9)\r\n",
        "    pil_image = transforms.ToPILImage()(image).convert('RGB')\r\n",
        "    new_image = transform_image(pil_image, cur_transform, magnitude)\r\n",
        "    return transforms.ToTensor()(new_image)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ylq2Y6L-bDta"
      },
      "source": [
        "**New Version of RandAugment**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HE8ZPisWbEss"
      },
      "source": [
        "TRANSFORMATIONS = ['invert', 'cutout', 'sharpness', 'autocontrast', 'posterize', 'shearx', 'translatex', 'translatey',\r\n",
        "                   'sheary', 'rotate', 'equalize', 'contrast', 'color', 'solarize', 'brightness']"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eWh-CsNHbGUY"
      },
      "source": [
        "def new_transform_image(image, transform):\r\n",
        "    \"\"\"Applies the transform to the given image, using a custom random range for\r\n",
        "    the transform magnitude (if the transformation takes a magnitude).\r\n",
        "\r\n",
        "    Args:\r\n",
        "        image: The image to transform.\r\n",
        "        transform: The transformation to apply.\r\n",
        "    Returns:\r\n",
        "        The transformed image.\r\n",
        "    \"\"\"\r\n",
        "    if transform == 'invert':\r\n",
        "        return PIL.ImageOps.invert(image)\r\n",
        "    elif transform == 'cutout':\r\n",
        "        magnitude = random.randint(1, 8)\r\n",
        "        crop_box = (magnitude, magnitude, image.width - magnitude, image.height - magnitude)\r\n",
        "        return image.crop(box=crop_box).resize(image.size)\r\n",
        "    elif transform == 'sharpness':\r\n",
        "        enhancer = PIL.ImageEnhance.Sharpness(image)\r\n",
        "        magnitude = random.uniform(0.0, 2.0)\r\n",
        "        return enhancer.enhance(magnitude)\r\n",
        "    elif transform == 'autocontrast':\r\n",
        "        magnitude = random.randint(1, 35)\r\n",
        "        return PIL.ImageOps.autocontrast(image, cutoff=magnitude)\r\n",
        "    elif transform == 'posterize':\r\n",
        "        magnitude = random.randint(3, 8)\r\n",
        "        return PIL.ImageOps.posterize(image, magnitude)\r\n",
        "    elif transform == 'shearx':\r\n",
        "        magnitude = random.uniform(0.0, 0.5)\r\n",
        "        transform_values = (1, magnitude, -1 * magnitude * image.width, 0, 1, 0)\r\n",
        "        return image.transform(image.size, Image.AFFINE, transform_values)\r\n",
        "    elif transform == 'translatex':\r\n",
        "        magnitude = random.uniform(0.0, 10.0)\r\n",
        "        return image.transform(image.size, Image.AFFINE, (1, 0, magnitude, 0, 1, 0))\r\n",
        "    elif transform == 'translatey':\r\n",
        "        magnitude = random.uniform(0.0, 10.0)\r\n",
        "        return image.transform(image.size, Image.AFFINE, (1, 0, 0, 0, 1, magnitude))\r\n",
        "    elif transform == 'sheary':\r\n",
        "        magnitude = random.uniform(0.0, 0.5)\r\n",
        "        transform_values = (1, 0, 0, magnitude, 1, -1 * magnitude * image.height)\r\n",
        "        return image.transform(image.size, Image.AFFINE, transform_values)\r\n",
        "    elif transform == 'rotate':\r\n",
        "        magnitude = random.uniform(-10.0, 10.0)\r\n",
        "        return image.rotate(magnitude)\r\n",
        "    elif transform == 'equalize':\r\n",
        "        return PIL.ImageOps.equalize(image)\r\n",
        "    elif transform == 'contrast':\r\n",
        "        magnitude = random.uniform(0.3, 2.0)\r\n",
        "        enhancer = PIL.ImageEnhance.Contrast(image)\r\n",
        "        return enhancer.enhance(magnitude)\r\n",
        "    elif transform == 'color':\r\n",
        "        magnitude = random.uniform(0.0, 2.0)\r\n",
        "        enhancer = PIL.ImageEnhance.Color(image)\r\n",
        "        return enhancer.enhance(magnitude)\r\n",
        "    elif transform == 'solarize':\r\n",
        "        magnitude = random.randint(128, 256)\r\n",
        "        return PIL.ImageOps.solarize(image, magnitude)\r\n",
        "    else:\r\n",
        "        # brightness\r\n",
        "        magnitude = random.uniform(0.3, 2.0)\r\n",
        "        enhancer = PIL.ImageEnhance.Brightness(image)\r\n",
        "        return enhancer.enhance(magnitude)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f7cEyhZbbh5T"
      },
      "source": [
        "def new_rand_augment(image, probability=0.5):\r\n",
        "    \"\"\"Applies a modified version of RandAugment to the given image (a tensor)\r\n",
        "    with the given probability being the probability with which to apply the\r\n",
        "    transformation.\r\n",
        "\r\n",
        "    Args:\r\n",
        "        image: The image to transform.\r\n",
        "        probability: The probability with which to transform the image.\r\n",
        "    Returns:\r\n",
        "        The transformed image.\r\n",
        "    \"\"\"\r\n",
        "    if np.random.binomial(1, 0.5) == 0:\r\n",
        "        return torch.clone(image)\r\n",
        "    cur_transform = random.choice(TRANSFORMATIONS)\r\n",
        "    pil_image = transforms.ToPILImage()(image).convert('RGB')\r\n",
        "    new_image = new_transform_image(pil_image, cur_transform)\r\n",
        "    return transforms.ToTensor()(new_image)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SQeYxkQ1cJ06"
      },
      "source": [
        "**Helper function to enable storing augmented image tensors in a dictionary, where the key is the hash of the original image tensor.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bdFGXxYfcL9t"
      },
      "source": [
        "def hash_tensor(input_tensor):\r\n",
        "    \"\"\"Hashes the tensor.\r\n",
        "\r\n",
        "    Args:\r\n",
        "        input_tensor: Tensor to hash.\r\n",
        "    Returns:\r\n",
        "        The hash value of the given tensor.\r\n",
        "    \"\"\"\r\n",
        "    input_multiply_factor = 3\r\n",
        "    input_multiplier = torch.arange(1, input_tensor.size()[1] + 1) * input_multiply_factor\r\n",
        "    return torch.sum(input_tensor.to(device) * input_multiplier.to(device)).item()"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dHhNVrFycNmX"
      },
      "source": [
        "**Mount Google Drive**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KQ98f398tYP3"
      },
      "source": [
        "from google.colab import drive\r\n",
        "drive.mount('/gdrive/')\r\n",
        "!ls /gdrive"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r1Avp2-DtY9X"
      },
      "source": [
        "**The general setup, training, and evaluation code below is from [CSE 543 Deep Learning Homework 1](https://github.com/pjreddie/uwnet/blob/master/hw1.ipynb)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2SF1R3y0tjK6"
      },
      "source": [
        "import os\r\n",
        "\r\n",
        "BASE_PATH = '/gdrive/My Drive/599project/cv/'\r\n",
        "if not os.path.exists(BASE_PATH):\r\n",
        "    os.makedirs(BASE_PATH)\r\n",
        "DATA_PATH = BASE_PATH + 'tiny_imagenet/'\r\n",
        "\r\n",
        "!pwd\r\n",
        "!ls\r\n",
        "os.chdir(BASE_PATH)\r\n",
        "if not os.path.exists(DATA_PATH + 'train.h5'):\r\n",
        "    !wget https://courses.cs.washington.edu/courses/cse599g1/19au/files/homework2.tar\r\n",
        "    !tar -xvf homework2.tar\r\n",
        "    !rm homework2.tar\r\n",
        "!pwd\r\n",
        "!ls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gJ0F74WmcbZB"
      },
      "source": [
        "import h5py\r\n",
        "import sys\r\n",
        "sys.path.append(BASE_PATH)\r\n",
        "import pt_util"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vp7lI3OpaFUl"
      },
      "source": [
        "device = torch.device(\"cuda\")"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xHf7mVcIvBfD"
      },
      "source": [
        "import glob\r\n",
        "import re\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "try:\r\n",
        "    # For 2.7\r\n",
        "    import cPickle as pickle\r\n",
        "except:\r\n",
        "    # For 3.x\r\n",
        "    import pickle\r\n",
        "\r\n",
        "\r\n",
        "def restore(net, save_file):\r\n",
        "    \"\"\"Restores the weights from a saved file\r\n",
        "\r\n",
        "    This does more than the simple Pytorch restore. It checks that the names\r\n",
        "    of variables match, and if they don't doesn't throw a fit. It is similar\r\n",
        "    to how Caffe acts. This is especially useful if you decide to change your\r\n",
        "    network architecture but don't want to retrain from scratch.\r\n",
        "\r\n",
        "    Args:\r\n",
        "        net(torch.nn.Module): The net to restore\r\n",
        "        save_file(str): The file path\r\n",
        "    \"\"\"\r\n",
        "\r\n",
        "    net_state_dict = net.state_dict()\r\n",
        "    restore_state_dict = torch.load(save_file)\r\n",
        "\r\n",
        "    restored_var_names = set()\r\n",
        "\r\n",
        "    print('Restoring:')\r\n",
        "    for var_name in restore_state_dict.keys():\r\n",
        "        if var_name in net_state_dict:\r\n",
        "            var_size = net_state_dict[var_name].size()\r\n",
        "            restore_size = restore_state_dict[var_name].size()\r\n",
        "            if var_size != restore_size:\r\n",
        "                print('Shape mismatch for var', var_name, 'expected', var_size, 'got', restore_size)\r\n",
        "            else:\r\n",
        "                if isinstance(net_state_dict[var_name], torch.nn.Parameter):\r\n",
        "                    # backwards compatibility for serialized parameters\r\n",
        "                    net_state_dict[var_name] = restore_state_dict[var_name].data\r\n",
        "                try:\r\n",
        "                    net_state_dict[var_name].copy_(restore_state_dict[var_name])\r\n",
        "                    print(str(var_name) + ' -> \\t' + str(var_size) + ' = ' + str(int(np.prod(var_size) * 4 / 10**6)) + 'MB')\r\n",
        "                    restored_var_names.add(var_name)\r\n",
        "                except Exception as ex:\r\n",
        "                    print('While copying the parameter named {}, whose dimensions in the model are'\r\n",
        "                          ' {} and whose dimensions in the checkpoint are {}, ...'.format(\r\n",
        "                              var_name, var_size, restore_size))\r\n",
        "                    raise ex\r\n",
        "\r\n",
        "    ignored_var_names = sorted(list(set(restore_state_dict.keys()) - restored_var_names))\r\n",
        "    unset_var_names = sorted(list(set(net_state_dict.keys()) - restored_var_names))\r\n",
        "    print('')\r\n",
        "    if len(ignored_var_names) == 0:\r\n",
        "        print('Restored all variables')\r\n",
        "    else:\r\n",
        "        print('Did not restore:\\n\\t' + '\\n\\t'.join(ignored_var_names))\r\n",
        "    if len(unset_var_names) == 0:\r\n",
        "        print('No new variables')\r\n",
        "    else:\r\n",
        "        print('Initialized but did not modify:\\n\\t' + '\\n\\t'.join(unset_var_names))\r\n",
        "\r\n",
        "    print('Restored %s' % save_file)\r\n",
        "\r\n",
        "\r\n",
        "def restore_latest(net, folder):\r\n",
        "    \"\"\"Restores the most recent weights in a folder\r\n",
        "\r\n",
        "    Args:\r\n",
        "        net(torch.nn.module): The net to restore\r\n",
        "        folder(str): The folder path\r\n",
        "    Returns:\r\n",
        "        int: Attempts to parse the epoch from the state and returns it if possible. Otherwise returns 0.\r\n",
        "    \"\"\"\r\n",
        "\r\n",
        "    checkpoints = sorted(glob.glob(folder + '/*.pt'), key=os.path.getmtime)\r\n",
        "    start_it = 0\r\n",
        "    if len(checkpoints) > 0:\r\n",
        "        restore(net, checkpoints[-1])\r\n",
        "        try:\r\n",
        "            start_it = int(re.findall(r'\\d+', checkpoints[-1])[-1])\r\n",
        "        except:\r\n",
        "            pass\r\n",
        "    return start_it\r\n",
        "\r\n",
        "\r\n",
        "def save(net, file_name, num_to_keep=1):\r\n",
        "    \"\"\"Saves the net to file, creating folder paths if necessary.\r\n",
        "\r\n",
        "    Args:\r\n",
        "        net(torch.nn.module): The network to save\r\n",
        "        file_name(str): the path to save the file.\r\n",
        "        num_to_keep(int): Specifies how many previous saved states to keep once this one has been saved.\r\n",
        "            Defaults to 1. Specifying < 0 will not remove any previous saves.\r\n",
        "    \"\"\"\r\n",
        "\r\n",
        "    folder = os.path.dirname(file_name)\r\n",
        "    if not os.path.exists(folder):\r\n",
        "        os.makedirs(folder)\r\n",
        "    torch.save(net.state_dict(), file_name)\r\n",
        "    extension = os.path.splitext(file_name)[1]\r\n",
        "    checkpoints = sorted(glob.glob(folder + '/*' + extension), key=os.path.getmtime)\r\n",
        "    print('Saved %s\\n' % file_name)\r\n",
        "    if num_to_keep > 0:\r\n",
        "        for ff in checkpoints[:-num_to_keep]:\r\n",
        "            os.remove(ff)\r\n",
        "\r\n",
        "def write_log(filename, data):\r\n",
        "    \"\"\"Pickles and writes data to a file\r\n",
        "\r\n",
        "    Args:\r\n",
        "        filename(str): File name\r\n",
        "        data(pickleable object): Data to save\r\n",
        "    \"\"\"\r\n",
        "\r\n",
        "    if not os.path.exists(os.path.dirname(filename)):\r\n",
        "        os.makedirs(os.path.dirname(filename))\r\n",
        "    pickle.dump(data, open(filename, 'wb'))\r\n",
        "\r\n",
        "def read_log(filename, default_value=None):\r\n",
        "    \"\"\"Reads pickled data or returns the default value if none found\r\n",
        "\r\n",
        "    Args:\r\n",
        "        filename(str): File name\r\n",
        "        default_value(anything): Value to return if no file is found\r\n",
        "    Returns:\r\n",
        "        unpickled file\r\n",
        "    \"\"\"\r\n",
        "\r\n",
        "    if os.path.exists(filename):\r\n",
        "        return pickle.load(open(filename, 'rb'))\r\n",
        "    return default_value\r\n",
        "\r\n",
        "def show_images(images, titles=None, columns=5, max_rows=5):\r\n",
        "    \"\"\"Shows images in a tiled format\r\n",
        "\r\n",
        "    Args:\r\n",
        "        images(list[np.array]): Images to show\r\n",
        "        titles(list[string]): Titles for each of the images\r\n",
        "        columns(int): How many columns to use in the tiling\r\n",
        "        max_rows(int): If there are more than columns * max_rows images, only the first n of them will be shown.\r\n",
        "    \"\"\"\r\n",
        "\r\n",
        "    images = images[:min(len(images), max_rows * columns)]\r\n",
        "\r\n",
        "    plt.figure(figsize=(20, 10))\r\n",
        "    for ii, image in enumerate(images):\r\n",
        "        plt.subplot(len(images) / columns + 1, columns, ii + 1)\r\n",
        "        plt.axis('off')\r\n",
        "        if titles is not None and ii < len(titles):\r\n",
        "            plt.title(str(titles[ii]))\r\n",
        "        plt.imshow(image)\r\n",
        "    plt.show()\r\n",
        "\r\n",
        "def plot(x_values, y_values, title, xlabel, ylabel):\r\n",
        "    \"\"\"Plots a line graph\r\n",
        "\r\n",
        "    Args:\r\n",
        "        x_values(list or np.array): x values for the line\r\n",
        "        y_values(list or np.array): y values for the line\r\n",
        "        title(str): Title for the plot\r\n",
        "        xlabel(str): Label for the x axis\r\n",
        "        ylabel(str): label for the y axis\r\n",
        "    \"\"\"\r\n",
        "\r\n",
        "    plt.figure(figsize=(20, 10))\r\n",
        "    plt.plot(x_values, y_values)\r\n",
        "    plt.title(title)\r\n",
        "    plt.xlabel(xlabel)\r\n",
        "    plt.ylabel(ylabel)\r\n",
        "    plt.show()\r\n",
        "\r\n",
        "def to_scaled_uint8(array):\r\n",
        "    \"\"\"Returns a normalized uint8 scaled to 0-255. This is useful for showing images especially of floats.\r\n",
        "\r\n",
        "    Args:\r\n",
        "        array(np.array): The array to normalize\r\n",
        "    Returns:\r\n",
        "        np.array normalized and of type uint8\r\n",
        "    \"\"\"\r\n",
        "\r\n",
        "    array = np.array(array, dtype=np.float32)\r\n",
        "    array -= np.min(array)\r\n",
        "    array *= (255. / np.max(array))\r\n",
        "    array = array.astype(np.uint8)\r\n",
        "    return array"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6svkSDr6vsy8"
      },
      "source": [
        "**Dataset class:** To be used with PyTorch DataLoader to return a given image or the number of images."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DfJGn92hvu9Q"
      },
      "source": [
        "class ComputerVisionDataset(torch.utils.data.Dataset):\r\n",
        "    \"\"\"Dataset class to be used with Pytorch DataLoader to return a given image\r\n",
        "    or the number of images.\r\n",
        "    \"\"\"\r\n",
        "    def __init__(self, cv_data):\r\n",
        "        \"\"\"Stores the given image data to be accessed later.\r\n",
        "\r\n",
        "        Args:\r\n",
        "            cv_data: Image data.\r\n",
        "        \"\"\"\r\n",
        "        super(ComputerVisionDataset, self).__init__()\r\n",
        "\r\n",
        "        self.cv_data = cv_data\r\n",
        "\r\n",
        "    def __len__(self):\r\n",
        "        \"\"\"Returns the number of images in the dataset.\r\n",
        "\r\n",
        "        Returns:\r\n",
        "            The number of images in the dataset.\r\n",
        "        \"\"\"\r\n",
        "        return len(self.cv_data)\r\n",
        "        \r\n",
        "    def __getitem__(self, idx):\r\n",
        "        \"\"\"Returns the image at the given index.\r\n",
        "\r\n",
        "        Args:\r\n",
        "            idx: Index of the item to return.\r\n",
        "        Returns:\r\n",
        "            Returns the image at the given index.\r\n",
        "        \"\"\"       \r\n",
        "        return self.cv_data[idx][0], self.cv_data[idx][1]\r\n"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZZhSnJp4vNw8"
      },
      "source": [
        "**Wide-ResNet-28-2 Model** copied from https://github.com/xternalz/WideResNet-pytorch/blob/master/wideresnet.py since the model itself couldn't be automatically loaded in. The loss function is implemented according to the loss function for [UDA](https://arxiv.org/pdf/1904.12848.pdf)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vrz6eOudXWHY"
      },
      "source": [
        "class BasicBlock(nn.Module):\r\n",
        "    def __init__(self, in_planes, out_planes, stride, dropRate=0.0):\r\n",
        "        super(BasicBlock, self).__init__()\r\n",
        "        self.bn1 = nn.BatchNorm2d(in_planes)\r\n",
        "        self.relu1 = nn.ReLU(inplace=True)\r\n",
        "        self.conv1 = nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\r\n",
        "                               padding=1, bias=False)\r\n",
        "        self.bn2 = nn.BatchNorm2d(out_planes)\r\n",
        "        self.relu2 = nn.ReLU(inplace=True)\r\n",
        "        self.conv2 = nn.Conv2d(out_planes, out_planes, kernel_size=3, stride=1,\r\n",
        "                               padding=1, bias=False)\r\n",
        "        self.droprate = dropRate\r\n",
        "        self.equalInOut = (in_planes == out_planes)\r\n",
        "        self.convShortcut = (not self.equalInOut) and nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride,\r\n",
        "                               padding=0, bias=False) or None\r\n",
        "    def forward(self, x):\r\n",
        "        if not self.equalInOut:\r\n",
        "            x = self.relu1(self.bn1(x))\r\n",
        "        else:\r\n",
        "            out = self.relu1(self.bn1(x))\r\n",
        "        out = self.relu2(self.bn2(self.conv1(out if self.equalInOut else x)))\r\n",
        "        if self.droprate > 0:\r\n",
        "            out = F.dropout(out, p=self.droprate, training=self.training)\r\n",
        "        out = self.conv2(out)\r\n",
        "        return torch.add(x if self.equalInOut else self.convShortcut(x), out)\r\n",
        "\r\n",
        "class NetworkBlock(nn.Module):\r\n",
        "    def __init__(self, nb_layers, in_planes, out_planes, block, stride, dropRate=0.0):\r\n",
        "        super(NetworkBlock, self).__init__()\r\n",
        "        self.layer = self._make_layer(block, in_planes, out_planes, nb_layers, stride, dropRate)\r\n",
        "    def _make_layer(self, block, in_planes, out_planes, nb_layers, stride, dropRate):\r\n",
        "        layers = []\r\n",
        "        for i in range(int(nb_layers)):\r\n",
        "            layers.append(block(i == 0 and in_planes or out_planes, out_planes, i == 0 and stride or 1, dropRate))\r\n",
        "        return nn.Sequential(*layers)\r\n",
        "    def forward(self, x):\r\n",
        "        return self.layer(x)\r\n",
        "\r\n",
        "class WideResNet(nn.Module):\r\n",
        "    def __init__(self, depth, num_classes, widen_factor=1, dropRate=0.0):\r\n",
        "        super(WideResNet, self).__init__()\r\n",
        "        nChannels = [16, 16*widen_factor, 32*widen_factor, 64*widen_factor]\r\n",
        "        assert((depth - 4) % 6 == 0)\r\n",
        "        n = (depth - 4) / 6\r\n",
        "        block = BasicBlock\r\n",
        "        # 1st conv before any network block\r\n",
        "        self.conv1 = nn.Conv2d(3, nChannels[0], kernel_size=3, stride=1,\r\n",
        "                               padding=1, bias=False)\r\n",
        "        # 1st block\r\n",
        "        self.block1 = NetworkBlock(n, nChannels[0], nChannels[1], block, 1, dropRate)\r\n",
        "        # 2nd block\r\n",
        "        self.block2 = NetworkBlock(n, nChannels[1], nChannels[2], block, 2, dropRate)\r\n",
        "        # 3rd block\r\n",
        "        self.block3 = NetworkBlock(n, nChannels[2], nChannels[3], block, 2, dropRate)\r\n",
        "        # global average pooling and classifier\r\n",
        "        self.bn1 = nn.BatchNorm2d(nChannels[3])\r\n",
        "        self.relu = nn.ReLU(inplace=True)\r\n",
        "        self.fc = nn.Linear(nChannels[3], num_classes)\r\n",
        "        self.nChannels = nChannels[3]\r\n",
        "\r\n",
        "        self.accuracy = None\r\n",
        "\r\n",
        "        for m in self.modules():\r\n",
        "            if isinstance(m, nn.Conv2d):\r\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\r\n",
        "            elif isinstance(m, nn.BatchNorm2d):\r\n",
        "                m.weight.data.fill_(1)\r\n",
        "                m.bias.data.zero_()\r\n",
        "            elif isinstance(m, nn.Linear):\r\n",
        "                m.bias.data.zero_()\r\n",
        "    def forward(self, x):\r\n",
        "        out = self.conv1(x)\r\n",
        "        out = self.block1(out)\r\n",
        "        out = self.block2(out)\r\n",
        "        out = self.block3(out)\r\n",
        "        out = self.relu(self.bn1(out))\r\n",
        "        out = F.avg_pool2d(out, 8)\r\n",
        "        out = out.view(-1, self.nChannels)\r\n",
        "        return self.fc(out)\r\n",
        "\r\n",
        "    def loss(self, image_data, label, temperature, beta, only_sup_loss, augmented_data=None, reduction='mean'):\r\n",
        "        \"\"\"Computes the loss used in UDA.\r\n",
        "\r\n",
        "        Args:\r\n",
        "            image_data: Batch of original images.\r\n",
        "            label: Labels for all images in the batch.\r\n",
        "            temperature: The temperature to use in sharpening predictions.\r\n",
        "            beta: Confidence threshold for confidence-based masking.\r\n",
        "            only_sup_loss: True if only the supervised loss should be computed,\r\n",
        "                False otherwise.\r\n",
        "            augmented_data: Augmented versions of the unlabeled images.\r\n",
        "            reduction: Type of reduction to use when computing the loss (i.e.,\r\n",
        "                mean or sum).\r\n",
        "        Returns:\r\n",
        "            The supervised loss (loss on labeled data) plus the consistency loss\r\n",
        "            (loss on unlabeled data).\r\n",
        "        \"\"\"\r\n",
        "\r\n",
        "        if only_sup_loss:\r\n",
        "            prediction = F.softmax(self.forward(image_data), dim=1)\r\n",
        "            return F.cross_entropy(prediction, label.squeeze(), reduction=reduction)\r\n",
        "        \r\n",
        "        unlabeled_loss_val = torch.tensor(0.0, requires_grad=True).to(device)\r\n",
        "        labeled_loss_val = torch.tensor(0.0, requires_grad=True).to(device)\r\n",
        "        num_unlabeled = 0\r\n",
        "        num_labeled = 0\r\n",
        "\r\n",
        "        for i in range(0, len(image_data)):\r\n",
        "            cur_image = torch.unsqueeze(image_data[i], 0).to(device)\r\n",
        "            if label[i] >= 0:\r\n",
        "                # Data is labeled\r\n",
        "                cur_prediction = F.softmax(self.forward(cur_image), dim=1)\r\n",
        "                cur_label = torch.unsqueeze(label[i], 0)\r\n",
        "                labeled_loss_val += F.cross_entropy(cur_prediction, cur_label, reduction=reduction)\r\n",
        "                num_labeled += 1\r\n",
        "            else:\r\n",
        "                # Data is unlabeled\r\n",
        "                with torch.no_grad():\r\n",
        "                    num_unlabeled += 1\r\n",
        "                    cur_prediction = self.forward(cur_image)\r\n",
        "                softmax_prediction = F.softmax(cur_prediction, dim=1)\r\n",
        "                if torch.max(softmax_prediction) > beta:\r\n",
        "                    sharpened = F.log_softmax(cur_prediction / temperature, dim=1)\r\n",
        "                    augmented_images = augmented_data[hash_tensor(image_data[i])]\r\n",
        "                    cur_unlabeled_loss = torch.tensor(0.0, requires_grad=True).to(device)\r\n",
        "                    for image in augmented_images:\r\n",
        "                        augmented_image = torch.unsqueeze(image, 0).to(device)\r\n",
        "                        augmented_classification = F.log_softmax(self.forward(augmented_image), dim=1)\r\n",
        "                        cur_unlabeled_loss += F.kl_div(sharpened, augmented_classification, reduction='batchmean', log_target=True)\r\n",
        "                    unlabeled_loss_val += (cur_unlabeled_loss / len(augmented_images))\r\n",
        "\r\n",
        "        total_loss_val = torch.tensor(0.0, requires_grad=True).to(device)\r\n",
        "        if num_unlabeled > 0:\r\n",
        "            total_loss_val += (0.5 * (unlabeled_loss_val / num_unlabeled))\r\n",
        "        if num_labeled > 0:\r\n",
        "            total_loss_val += (labeled_loss_val / num_labeled)\r\n",
        "        return total_loss_val\r\n",
        "\r\n",
        "    def save_model(self, file_path, num_to_keep=1):\r\n",
        "        \"\"\"Saves the model to the given file.\r\n",
        "\r\n",
        "        Args:\r\n",
        "            file_path: Path to save the model.\r\n",
        "            num_to_keep: Number of previous saved states to keep.\r\n",
        "        \"\"\"\r\n",
        "        pt_util.save(self, file_path, num_to_keep)\r\n",
        "        \r\n",
        "    def save_best_model(self, accuracy, file_path, num_to_keep=1):\r\n",
        "        \"\"\"If a higher accuracy is achieved, saves the current model.\r\n",
        "\r\n",
        "        Args:\r\n",
        "            accuracy: New accuracy achieved.\r\n",
        "            file_path: Path to save the model.\r\n",
        "            num_to_keep: Number of previous saved states to keep.\r\n",
        "        \"\"\"\r\n",
        "        if self.accuracy == None or accuracy > self.accuracy:\r\n",
        "            self.accuracy = accuracy\r\n",
        "            self.save_model(file_path, num_to_keep)\r\n",
        "\r\n",
        "    def load_model(self, file_path):\r\n",
        "        \"\"\"Loads the model from the given file.\r\n",
        "\r\n",
        "        Args:\r\n",
        "            file_path: Path to save the model.\r\n",
        "        \"\"\"\r\n",
        "        pt_util.restore(self, file_path)\r\n",
        "\r\n",
        "    def load_last_model(self, dir_path):\r\n",
        "        \"\"\"Loads the most recent model from the given directory.\r\n",
        "\r\n",
        "        Args:\r\n",
        "            dir_path: Directory from which to load the model.\r\n",
        "        \"\"\"\r\n",
        "        return pt_util.restore_latest(self, dir_path)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DDxWBa0NvQ0n"
      },
      "source": [
        "**Train and test**: This code is adapted from [CSE 543 Deep Learning Homework 1](https://github.com/pjreddie/uwnet/blob/master/hw1.ipynb)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zk5SAzhMvQEl"
      },
      "source": [
        "import time\r\n",
        "def train(model, device, train_loader, optimizer, epoch, log_interval, temperature, beta, augmented_images, pretrain):\r\n",
        "    \"\"\"Trains the model on the given data.\r\n",
        "\r\n",
        "    Args:\r\n",
        "        model: The model to train.\r\n",
        "        device: The device the model is on.\r\n",
        "        train_loader: DataLoader for the training data.\r\n",
        "        optimizer: Optimizer to use with training.\r\n",
        "        epoch: The current epoch.\r\n",
        "        log_interval: How often to log metrics.\r\n",
        "        temperature: The temperature to use in sharpening predictions.\r\n",
        "        beta: Confidence threshold for confidence-based masking.\r\n",
        "        augmented_images: Augmented versions of the unlabeled images.\r\n",
        "        pretrain: True if pretraining the model, False otherwise.\r\n",
        "    Returns:\r\n",
        "        The average loss for each batch for the current epoch.\r\n",
        "    \"\"\"\r\n",
        "    model.train()\r\n",
        "    losses = []\r\n",
        "    for batch_idx, (data, label) in enumerate(train_loader):\r\n",
        "        data, label = data.to(device), label.to(device)\r\n",
        "        optimizer.zero_grad()\r\n",
        "        loss = model.loss(data, label, temperature, beta, pretrain, augmented_data=augmented_images)\r\n",
        "        losses.append(loss.item())\r\n",
        "        loss.backward()\r\n",
        "        optimizer.step()\r\n",
        "        if batch_idx % log_interval == 0:\r\n",
        "            print('{} Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\r\n",
        "                time.ctime(time.time()),\r\n",
        "                epoch, batch_idx * len(data), len(train_loader.dataset),\r\n",
        "                100. * batch_idx / len(train_loader), loss.item()))\r\n",
        "    return np.mean(losses)\r\n",
        "\r\n",
        "def test(model, device, test_loader, temperature, beta, log_interval=None):\r\n",
        "    \"\"\"Evaluates the model on the test data.\r\n",
        "\r\n",
        "    Args:\r\n",
        "        model: The model to evaluate.\r\n",
        "        device: The device the model is on.\r\n",
        "        test_loader: DataLoader for the test data.\r\n",
        "        temperature: The temperature to use in sharpening predictions.\r\n",
        "        beta: Confidence threshold for confidence-based masking.\r\n",
        "        log_interval: How often to log metrics.\r\n",
        "    Returns:\r\n",
        "        The loss and accuracy on the test data.\r\n",
        "    \"\"\"\r\n",
        "    model.eval()\r\n",
        "    test_loss = 0\r\n",
        "    correct = 0\r\n",
        "\r\n",
        "    with torch.no_grad():\r\n",
        "        for batch_idx, (data, label) in enumerate(test_loader):\r\n",
        "            data, label = data.to(device), label.to(device)\r\n",
        "            output = F.softmax(model(data), dim=1)\r\n",
        "            test_loss_on = model.loss(data, label, temperature, beta, True, reduction='sum').item()\r\n",
        "            test_loss += test_loss_on\r\n",
        "            pred = output.max(1)[1]\r\n",
        "            correct_mask = pred.eq(label.view_as(pred))\r\n",
        "            num_correct = correct_mask.sum().item()\r\n",
        "            correct += num_correct\r\n",
        "            if log_interval is not None and batch_idx % log_interval == 0:\r\n",
        "                print('{} Test: [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\r\n",
        "                    time.ctime(time.time()),\r\n",
        "                    batch_idx * len(data), len(test_loader.dataset),\r\n",
        "                    100. * batch_idx / len(test_loader), test_loss_on))\r\n",
        "\r\n",
        "    test_loss /= len(test_loader.dataset)\r\n",
        "    test_accuracy = 100. * correct / len(test_loader.dataset)\r\n",
        "\r\n",
        "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\r\n",
        "        test_loss, correct, len(test_loader.dataset), test_accuracy))\r\n",
        "    return test_loss, test_accuracy"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7yYurf77vVXo"
      },
      "source": [
        "def run_training(data_train, data_test, data_path, hyperparams, experiment_num, class_names, augmented_images, pretrain):\r\n",
        "    \"\"\"Train and evaluate the model on the given training and test data, respectively.\r\n",
        "\r\n",
        "    Args:\r\n",
        "        data_train: The training data.\r\n",
        "        data_test: The test data.\r\n",
        "        data_path: Location of the image data.\r\n",
        "        hyperparams: Hyperparameters to use during training.\r\n",
        "        experiment_num: Current experiment number.\r\n",
        "        class_names: Names of the classes for the images.\r\n",
        "        augmented_images: Augmented versions of the training data images.\r\n",
        "        pretrain: True if pretraining the model, False otherwise.\r\n",
        "    \"\"\"\r\n",
        "    BATCH_SIZE = hyperparams['batch_size']\r\n",
        "    TEST_BATCH_SIZE = hyperparams['test_batch_size']\r\n",
        "    EPOCHS = hyperparams['epochs']\r\n",
        "    LEARNING_RATE = hyperparams['learning_rate']\r\n",
        "    MOMENTUM = hyperparams['momentum']\r\n",
        "    WEIGHT_DECAY = hyperparams['weight_decay']\r\n",
        "    TEMPERATURE = hyperparams['temperature']\r\n",
        "    BETA = hyperparams['beta']\r\n",
        "    USE_CUDA = True\r\n",
        "    SEED = 0\r\n",
        "    PRINT_INTERVAL = 100\r\n",
        "\r\n",
        "    EXPERIMENT_VERSION = experiment_num\r\n",
        "    LOG_PATH = data_path + 'logs/' + EXPERIMENT_VERSION + '/'\r\n",
        "    use_cuda = USE_CUDA and torch.cuda.is_available()\r\n",
        "\r\n",
        "    print('Using device', device)\r\n",
        "    import multiprocessing\r\n",
        "    print('num cpus:', multiprocessing.cpu_count())\r\n",
        "\r\n",
        "    kwargs = {'num_workers': multiprocessing.cpu_count(),\r\n",
        "              'pin_memory': True} if use_cuda else {}\r\n",
        "\r\n",
        "    train_loader = torch.utils.data.DataLoader(data_train, batch_size=BATCH_SIZE,\r\n",
        "                                              shuffle=True, **kwargs)\r\n",
        "\r\n",
        "    test_loader = torch.utils.data.DataLoader(data_test, batch_size=TEST_BATCH_SIZE,\r\n",
        "                                              shuffle=False, **kwargs)\r\n",
        "\r\n",
        "    model = WideResNet(28, len(class_names), widen_factor=2).to(device)\r\n",
        "    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\r\n",
        "    start_epoch = model.load_last_model(LOG_PATH)\r\n",
        "\r\n",
        "    train_losses, test_losses, test_accuracies = pt_util.read_log(LOG_PATH + 'log.pkl', ([], [], []))\r\n",
        "    test_loss, test_accuracy = test(model, device, test_loader, TEMPERATURE, BETA)\r\n",
        "\r\n",
        "    test_losses.append((start_epoch, test_loss))\r\n",
        "    test_accuracies.append((start_epoch, test_accuracy))\r\n",
        "\r\n",
        "    try:\r\n",
        "        for epoch in range(start_epoch, EPOCHS + 1):\r\n",
        "            train_loss = train(model, device, train_loader, optimizer, epoch, PRINT_INTERVAL, TEMPERATURE, BETA, augmented_images, pretrain)\r\n",
        "            test_loss, test_accuracy = test(model, device, test_loader, TEMPERATURE, BETA)\r\n",
        "            train_losses.append((epoch, train_loss))\r\n",
        "            test_losses.append((epoch, test_loss))\r\n",
        "            test_accuracies.append((epoch, test_accuracy))\r\n",
        "            pt_util.write_log(LOG_PATH + 'log.pkl', (train_losses, test_losses, test_accuracies))\r\n",
        "            model.save_best_model(test_accuracy, LOG_PATH + '%03d.pt' % epoch)\r\n",
        "\r\n",
        "\r\n",
        "    except KeyboardInterrupt as ke:\r\n",
        "        print('Interrupted')\r\n",
        "    except:\r\n",
        "        import traceback\r\n",
        "        traceback.print_exc()\r\n",
        "    finally:\r\n",
        "        model.save_model(LOG_PATH + '%03d.pt' % epoch, 0)\r\n",
        "        ep, val = zip(*train_losses)\r\n",
        "        pt_util.plot(ep, val, 'Train loss', 'Epoch', 'Error')\r\n",
        "        ep, val = zip(*test_losses)\r\n",
        "        pt_util.plot(ep, val, 'Test loss', 'Epoch', 'Error')\r\n",
        "        ep, val = zip(*test_accuracies)\r\n",
        "        pt_util.plot(ep, val, 'Test accuracy', 'Epoch', 'Error')\r\n"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m0EmJxj3a8dz"
      },
      "source": [
        "**Helper function to convert images to tensors**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PqEseagmaUDY"
      },
      "source": [
        "def convert_images_to_tensor(image_data):\r\n",
        "    \"\"\"Convert each image in the list to a tensor\r\n",
        "\r\n",
        "    Args:\r\n",
        "        image_data: A list of tuples containing an image and the label.\r\n",
        "    Returns:\r\n",
        "        A list of tuples with each image converted to a tensor.\r\n",
        "    \"\"\"\r\n",
        "    tensor_data = []\r\n",
        "    for image in image_data:\r\n",
        "        tensor_image = transforms.ToTensor()(image[0])\r\n",
        "        tensor_data.append((tensor_image, image[1]))\r\n",
        "    return tensor_data"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-aS9lMi5bgG1"
      },
      "source": [
        "**Experiments with CIFAR**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "voa6zMmqSZPr"
      },
      "source": [
        "**Note:** When trying to download the data, if you get an error saying \"Transport Error: not connected to Drive\", then re-run the cell to mount Google Drive and try running the cell to download the data again."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "962D9YCDdy5I"
      },
      "source": [
        "Load the CIFAR data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0mFKw1kJbH3S"
      },
      "source": [
        "cifar_data_path = BASE_PATH + 'cifar/'\r\n",
        "original_cifar_data_train = datasets.CIFAR10(root=cifar_data_path, train=True, download=True)\r\n",
        "cifar_data_test = datasets.CIFAR10(root=cifar_data_path, train=False, download=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qr40ha85d3DV"
      },
      "source": [
        "Randomly select a portion of the data to be labeled images and the remainder to be unlabeled"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZvdCvdrmxjBm"
      },
      "source": [
        "shuffled_cifar_data_train = list(original_cifar_data_train)\r\n",
        "random.shuffle(shuffled_cifar_data_train)\r\n",
        "num_labeled = 4000\r\n",
        "labeled_cifar_data_train = shuffled_cifar_data_train[:num_labeled]\r\n",
        "unlabeled_cifar_data_train = [(image[0], -1) for image in shuffled_cifar_data_train[num_labeled:]]\r\n",
        "full_cifar_data_train = labeled_cifar_data_train + unlabeled_cifar_data_train\r\n",
        "random.shuffle(full_cifar_data_train)\r\n",
        "new_cifar_data_train = ComputerVisionDataset(convert_images_to_tensor(full_cifar_data_train))"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zUEBjH3Qd_rn"
      },
      "source": [
        "Create augmented versions of all unlabeled CIFAR images in the training data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tMesodW2a1Fc"
      },
      "source": [
        "num_augmented_images = 5\r\n",
        "augmented_cifar_images = dict()\r\n",
        "for cifar_image in new_cifar_data_train:\r\n",
        "    if cifar_image[1] < 0:\r\n",
        "        hashed_image = hash_tensor(cifar_image[0])\r\n",
        "        new_images = []\r\n",
        "        for i in range(num_augmented_images):\r\n",
        "            # To use the modified version of RandAugment, call the function\r\n",
        "            # new_rand_augment instead of rand_augment\r\n",
        "            new_images.append(rand_augment(cifar_image[0]))\r\n",
        "        augmented_cifar_images[hashed_image] = new_images"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CU_mZ9Qsbt1l"
      },
      "source": [
        "cifar_data_test_tensor = ComputerVisionDataset(convert_images_to_tensor(cifar_data_test))"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rWvn7rhtbWPU"
      },
      "source": [
        "cifar_class_names = ['plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "99NbGm7UslRr"
      },
      "source": [
        "cifar_experiment_num = \"0.001\""
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9aa2wk7nKTTB"
      },
      "source": [
        "Pretrain with original CIFAR data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-jVZmEG_ZNy9"
      },
      "source": [
        "transform_train = transforms.Compose([\r\n",
        "    transforms.RandomResizedCrop(32),\r\n",
        "    transforms.RandomHorizontalFlip(),\r\n",
        "    transforms.RandomAffine(3),\r\n",
        "    transforms.RandomRotation(3),\r\n",
        "    transforms.ColorJitter(),\r\n",
        "    transforms.ToTensor(),\r\n",
        "])\r\n",
        "\r\n",
        "original_cifar_data_train_tensor = datasets.CIFAR10(root=cifar_data_path, train=True, download=True, transform=transform_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z6EU02dTbNa8"
      },
      "source": [
        "cifar_hyperparams = {\r\n",
        "    'batch_size': 32,\r\n",
        "    'test_batch_size': 10,\r\n",
        "    'epochs': 500,\r\n",
        "    'learning_rate': 0.001,\r\n",
        "    'momentum': 0.9,\r\n",
        "    'weight_decay': 0.0,\r\n",
        "    'temperature': 0.4,\r\n",
        "    'beta': 1\r\n",
        "}\r\n",
        "run_training(original_cifar_data_train_tensor, cifar_data_test_tensor, cifar_data_path, cifar_hyperparams,\r\n",
        "             cifar_experiment_num, cifar_class_names, None, True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8U74qw7YKPXc"
      },
      "source": [
        "Train with augmented CIFAR data using UDA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "btsTn9gRxQb8"
      },
      "source": [
        "ssl_cifar_hyperparams = {\r\n",
        "    'batch_size': 64,\r\n",
        "    'test_batch_size': 10,\r\n",
        "    'epochs': 200,\r\n",
        "    'learning_rate': 0.0001,\r\n",
        "    'momentum': 0.9,\r\n",
        "    'weight_decay': 0.0005,\r\n",
        "    'temperature': 0.4,\r\n",
        "    'beta': 0.7\r\n",
        "}\r\n",
        "run_training(new_cifar_data_train, cifar_data_test_tensor, cifar_data_path, ssl_cifar_hyperparams,\r\n",
        "             cifar_experiment_num, cifar_class_names, augmented_cifar_images, False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eeNi0243bhsL"
      },
      "source": [
        "**Experiments with SVHN**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UATsFfiPebh4"
      },
      "source": [
        "Load the SVHN data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R5mgAi2bbkxQ"
      },
      "source": [
        "svhn_data_path = BASE_PATH + 'svhn/'\r\n",
        "original_svhn_data_train = datasets.SVHN(root=svhn_data_path, split='train', download=True)\r\n",
        "svhn_data_test = datasets.SVHN(root=svhn_data_path, split='test', download=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pEPrsER7glrN"
      },
      "source": [
        "Randomly select a portion of the data to be labeled images and the remainder to be unlabeled"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_OoJlcDJgmBG"
      },
      "source": [
        "shuffled_svhn_data_train = list(original_svhn_data_train)\r\n",
        "random.shuffle(shuffled_svhn_data_train)\r\n",
        "num_labeled = 1000\r\n",
        "labeled_svhn_data_train = shuffled_svhn_data_train[:num_labeled]\r\n",
        "unlabeled_svhn_data_train = [(image[0], -1) for image in shuffled_svhn_data_train[num_labeled:]]\r\n",
        "full_svhn_data_train = labeled_svhn_data_train + unlabeled_svhn_data_train\r\n",
        "random.shuffle(full_svhn_data_train)\r\n",
        "new_svhn_data_train = ComputerVisionDataset(convert_images_to_tensor(full_svhn_data_train))"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UV53YoHRguCj"
      },
      "source": [
        "Create augmented versions of all unlabeled SVHN images in the training data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "trX88-TrguU3"
      },
      "source": [
        "num_augmented_images = 5\r\n",
        "augmented_svhn_images = dict()\r\n",
        "for svhn_image in new_svhn_data_train:\r\n",
        "    if svhn_image[1] < 0:\r\n",
        "        hashed_image = hash_tensor(svhn_image[0])\r\n",
        "        new_images = []\r\n",
        "        for i in range(num_augmented_images):\r\n",
        "            # To use the modified version of RandAugment, call the function\r\n",
        "            # new_rand_augment instead of rand_augment\r\n",
        "            new_images.append(rand_augment(svhn_image[0]))\r\n",
        "        augmented_svhn_images[hashed_image] = new_images"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J_bWYJa1gz1X"
      },
      "source": [
        "svhn_data_test_tensor = ComputerVisionDataset(convert_images_to_tensor(svhn_data_test))"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rR7-AP9Eg1Ms"
      },
      "source": [
        "svhn_class_names = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VFxrYm1Pg1ed"
      },
      "source": [
        "svhn_experiment_num = \"0.001\""
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q9sgKyy9g64v"
      },
      "source": [
        "Pretrain with original SVHN data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cFN4zz2fg-TB"
      },
      "source": [
        "transform_train = transforms.Compose([\r\n",
        "    transforms.ToTensor(),\r\n",
        "])\r\n",
        "\r\n",
        "original_svhn_data_train_tensor = datasets.SVHN(root=svhn_data_path, split='train', download=True, transform=transform_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JsmTQ1WuhA_J"
      },
      "source": [
        "svhn_hyperparams = {\r\n",
        "    'batch_size': 64,\r\n",
        "    'test_batch_size': 10,\r\n",
        "    'epochs': 50,\r\n",
        "    'learning_rate': 0.0001,\r\n",
        "    'momentum': 0.9,\r\n",
        "    'weight_decay': 0.0,\r\n",
        "    'temperature': 0.4,\r\n",
        "    'beta': 1\r\n",
        "}\r\n",
        "run_training(original_svhn_data_train_tensor, svhn_data_test_tensor, svhn_data_path, svhn_hyperparams,\r\n",
        "             svhn_experiment_num, svhn_class_names, None, True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g3sRvekjhG_O"
      },
      "source": [
        "Train with augmented SVHN data using UDA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jj6i0-k8hJNw"
      },
      "source": [
        "ssl_svhn_hyperparams = {\r\n",
        "    'batch_size': 64,\r\n",
        "    'test_batch_size': 10,\r\n",
        "    'epochs': 100,\r\n",
        "    'learning_rate': 0.0001,\r\n",
        "    'momentum': 0.9,\r\n",
        "    'weight_decay': 0.0005,\r\n",
        "    'temperature': 0.4,\r\n",
        "    'beta': 0.8\r\n",
        "}\r\n",
        "run_training(new_svhn_data_train, svhn_data_test_tensor, svhn_data_path, ssl_svhn_hyperparams,\r\n",
        "             svhn_experiment_num, svhn_class_names, augmented_svhn_images, False)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}